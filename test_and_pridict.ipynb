{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import dgl\n",
    "import ast\n",
    "from model import MCDHGN\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn.pytorch import GATConv\n",
    "import logging \n",
    "import datetime\n",
    "from model import MySampler\n",
    "from utils import EarlyStopping, setup_seed, generate_traning_batch,HeteroDotProductPredictor\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score, auc, precision_recall_curve,roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "dgl.random.seed(42)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_paths = [['gg'],['ga','ag'],['gb','bg'],['gc','cg'],['gd','dg'],['ge','eg'],['gf','fg'],['gh','hg'],['gi','ig']]\n",
    "graph_path = './data/network/hetero/new_9nodes_graph.bin'\n",
    "graphs,_ = dgl.data.utils.load_graphs(graph_path)\n",
    "g = graphs[0]\n",
    "features = g.ndata['feature']['Gene']\n",
    "#In order to ensure reproducible operation, in addition to fixing the random number seed, I also fixed the intermediate random walk sampling results and the division results of the training set, verification set, and test set.\n",
    "# you also can get the random sampler result by the codes below.\n",
    "# gene_ids = g.nodes('Gene')\n",
    "# my_sampler = MySampler(g, meta_paths, 128)\n",
    "# _,gs = my_sampler.sample_blocks(gene_ids)\n",
    "# print(gs)\n",
    "gs = []\n",
    "for i in range(9):\n",
    "    loadpath = './Intermediate/blocks/{}gs.bin'.format(i)\n",
    "    with open(loadpath,'rb')as f:\n",
    "        tg = pickle.load(f)\n",
    "        gs.append(tg)\n",
    "# you can generate another train \\ val \\ test \\ set by the codes below\n",
    "# postivefile = './data/label/now_pos427.pkl'\n",
    "# negtivefile = './data/label/now_neg427.pkl'\n",
    "# train_batch,test_mask,test_label = generate_traning_batch(postivefile,negtivefile)\n",
    "test_mask = torch.load('./Intermediate/label_set/mydatatest_mask.pt')\n",
    "test_label = torch.load('./Intermediate/label_set/mydatatest_label.pt')\n",
    "with open('./Intermediate/label_set/train_batch.pkl','rb')as f:\n",
    "    train_batch = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n",
      "Device name: Tesla V100S-PCIE-32GB\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(device))\n",
    "else:\n",
    "    print('No GPU available, using CPU instead.')\n",
    "gs1 = [block.to(device) for block in gs]\n",
    "features = features.to(device)\n",
    "\n",
    "def evaluate(g,model, mask,label):\n",
    "    model.eval() \n",
    "    with torch.no_grad():\n",
    "        _,pred,alpha,beta = model(g,features)\n",
    "        output = pred[mask]\n",
    "        output = F.log_softmax(output,dim=1)\n",
    "        val_loss = F.nll_loss(output, label)\n",
    "        output = output.cpu().numpy()\n",
    "        label = label.cpu().numpy()\n",
    "        acc = accuracy_score(label, np.argmax(output, axis=1))\n",
    "        output = torch.sigmoid(pred[mask]).cpu().detach().numpy()\n",
    "        auc = roc_auc_score(label, output[:, 1])\n",
    "        aupr = average_precision_score(label, output[:, 1])\n",
    "    return val_loss,acc,auc,aupr,pred.cpu(),alpha,beta\n",
    "def generate_curve(fpred,test_mask,test_label):\n",
    "    label = test_label.cpu()\n",
    "    mask = test_mask.cpu()\n",
    "    probas = torch.nn.Sigmoid()(fpred.cpu())\n",
    "    probas = probas.detach().numpy()\n",
    "    fpr,tpr,_ = roc_curve(label, probas[mask][:,1])\n",
    "    p,r,_ = precision_recall_curve(label, probas[mask][:,1])\n",
    "    myauc = auc(fpr,tpr)\n",
    "    myaupr = auc(r,p)\n",
    "    return myauc,myaupr,fpr,tpr,p,r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6249895885390638 0.43792229766222146 0.44891640866873067\n",
      "10 0.5071630851241047 0.34413896278679545 0.6408668730650154\n",
      "20 0.7194111277694487 0.5730146149634641 0.695046439628483\n",
      "30 0.7681575878727303 0.6307111542419118 0.653250773993808\n",
      "40 0.7616296018657338 0.6129125170906872 0.6996904024767802\n",
      "50 0.8012972680326503 0.6847626639768907 0.7414860681114551\n",
      "60 0.7926661669165418 0.6757923788086279 0.6873065015479877\n",
      "70 0.8141970681326003 0.7214071303595105 0.7678018575851393\n",
      "80 0.7954356155255705 0.6872938753723831 0.6888544891640866\n",
      "90 0.8160919540229885 0.7318726557630653 0.760061919504644\n",
      "100 0.8162064800932868 0.7304386064015758 0.7275541795665634\n",
      "110 0.8235986173579878 0.7358267426219179 0.7445820433436533\n",
      "120 0.8262639513576545 0.7317914252258212 0.7337461300309598\n",
      "130 0.8177681992337165 0.7233530763873729 0.7708978328173375\n",
      "140 0.8266804097951023 0.7361211919800152 0.7492260061919505\n",
      "150 0.8354780942861902 0.7491559600637833 0.7476780185758514\n",
      "160 0.8386744127936032 0.7534633071411234 0.7260061919504643\n",
      "170 0.8329376978177576 0.7493878349037985 0.7198142414860681\n",
      "180 0.8288251707479594 0.740084219206614 0.7291021671826625\n",
      "190 0.833052223888056 0.7526753685826735 0.7492260061919505\n",
      "200 0.8261077794436114 0.7410137498337303 0.7647058823529411\n",
      "210 0.8389659336998168 0.7530214588042592 0.7585139318885449\n",
      "220 0.8363943028485757 0.7491442555131355 0.7739938080495357\n",
      "230 0.8332604531067799 0.7456769557968785 0.7678018575851393\n",
      "240 0.8352386306846576 0.7541193183807948 0.7739938080495357\n",
      "250 0.8414646843245045 0.7565943135820437 0.7693498452012384\n",
      "260 0.8349575212393803 0.7470140521841111 0.7523219814241486\n",
      "270 0.8376228552390471 0.7529812835970897 0.7662538699690402\n",
      "280 0.8278256704980844 0.7467684266952793 0.7662538699690402\n",
      "290 0.8396739130434782 0.7508988804214183 0.7523219814241486\n",
      "300 0.8400591370981176 0.7507000901679936 0.7662538699690402\n",
      "310 0.8350199900049975 0.7506709437656724 0.7476780185758514\n",
      "320 0.8423600699650176 0.753802795744673 0.7910216718266254\n",
      "330 0.8312614526070298 0.7511063448439846 0.7507739938080495\n",
      "340 0.837664501082792 0.7514571708089222 0.7755417956656346\n",
      "350 0.8459728469098784 0.7584791279483293 0.7693498452012384\n",
      "360 0.8436302681992337 0.7606071920123936 0.7894736842105263\n",
      "370 0.8383933033483257 0.7473498699099925 0.7755417956656346\n",
      "380 0.8401736631684157 0.7476941700811707 0.7724458204334366\n",
      "390 0.839923788105947 0.7552300087575423 0.781733746130031\n",
      "400 0.8410065800433117 0.7610971055786395 0.7708978328173375\n",
      "410 0.8445152423788105 0.7634375537044509 0.7879256965944272\n",
      "420 0.8445152423788107 0.7615088909915714 0.7987616099071208\n",
      "430 0.8481384307846076 0.769503509201203 0.7910216718266254\n",
      "440 0.8435782108945528 0.7651133782231092 0.7832817337461301\n",
      "450 0.8403714809262036 0.7634482108512215 0.7770897832817337\n",
      "460 0.839975845410628 0.7587027485951312 0.7848297213622291\n",
      "470 0.8426203564884224 0.7630457580742553 0.7538699690402477\n",
      "480 0.8465246543394969 0.7681522551848959 0.7786377708978328\n",
      "490 0.8456500916208564 0.7690531197529322 0.7631578947368421\n",
      "500 0.8436615025820423 0.7613285863473522 0.7693498452012384\n",
      "510 0.8436719140429786 0.7647632126966584 0.8018575851393189\n",
      "520 0.8478469098783941 0.7690243363678023 0.7848297213622291\n",
      "530 0.8477740296518408 0.769095555724996 0.7585139318885449\n",
      "540 0.845535565550558 0.7714556342381925 0.7910216718266254\n",
      "550 0.8456813260036647 0.7733718929621777 0.7925696594427245\n",
      "560 0.8475241545893719 0.7721388317062875 0.7739938080495357\n",
      "570 0.8445777111444278 0.7707361206134218 0.7693498452012384\n",
      "580 0.8498042645343995 0.779759632317924 0.7941176470588235\n",
      "590 0.8475970348159255 0.7761919117862469 0.7383900928792569\n",
      "600 0.8524487756121939 0.774736095422205 0.7708978328173375\n",
      "610 0.8467953523238381 0.7688534039380707 0.781733746130031\n",
      "620 0.8466391804097951 0.769955917302037 0.8065015479876161\n",
      "630 0.8507100616358486 0.773844107249689 0.7708978328173375\n",
      "640 0.8474096285190738 0.7735865404170823 0.7925696594427245\n",
      "650 0.8483778943861403 0.7723022841556266 0.7910216718266254\n",
      "660 0.8488047642845244 0.7708176266384448 0.7863777089783281\n",
      "670 0.8454522738630684 0.7717187620828756 0.7956656346749226\n",
      "680 0.8511056971514243 0.7781879672378703 0.7863777089783281\n",
      "690 0.8447651174412794 0.7704017959453707 0.7972136222910217\n",
      "700 0.8491066966516742 0.7735061402268624 0.7879256965944272\n",
      "710 0.8508766450108279 0.7723574779949685 0.7647058823529411\n",
      "720 0.8446610028319174 0.7714124763606091 0.7987616099071208\n",
      "730 0.8414126270198233 0.7562951552576426 0.7801857585139319\n",
      "740 0.8438176744960852 0.7496782342854036 0.5557275541795665\n",
      "750 0.7765388139263701 0.6012100117460413 0.7167182662538699\n",
      "760 0.7696256038647343 0.6134939577321219 0.3993808049535604\n",
      "770 0.8443174246210228 0.7548812604067882 0.7786377708978328\n",
      "780 0.8516575045810427 0.7595518303719746 0.7662538699690402\n",
      "790 0.8522717807762786 0.7615210688740448 0.7739938080495357\n",
      "800 0.8493669831750792 0.7539055155525378 0.8018575851393189\n",
      "810 0.8489192903548227 0.7582490440226151 0.7925696594427245\n",
      "820 0.8506684157921041 0.7685139531787514 0.7894736842105263\n",
      "830 0.854603948025987 0.773975755744684 0.7863777089783281\n",
      "840 0.8513659836748292 0.7721503188713381 0.7987616099071208\n",
      "850 0.8486173579876728 0.7685739753941316 0.7972136222910217\n",
      "860 0.8524487756121939 0.7715400656533337 0.7585139318885449\n",
      "870 0.8475658004331167 0.7706438553903997 0.8003095975232198\n",
      "880 0.8517407962685322 0.7704050192598764 0.7956656346749226\n",
      "890 0.853604447776112 0.7729629151449209 0.7770897832817337\n",
      "900 0.8530734632683658 0.7749829481955816 0.7863777089783281\n",
      "910 0.8523758953856405 0.7743073121556078 0.7832817337461301\n",
      "920 0.8553848075962018 0.781106205671322 0.7894736842105263\n",
      "930 0.8523758953856405 0.7777272810998169 0.7925696594427245\n",
      "940 0.852094785940363 0.7760630311774978 0.7956656346749226\n",
      "950 0.8550204064634349 0.7776614041017231 0.7956656346749226\n",
      "960 0.8547913543228386 0.7801824429972676 0.7925696594427245\n",
      "970 0.8568111777444611 0.7855035810712752 0.7956656346749226\n",
      "980 0.8559678494086291 0.7814205251361575 0.781733746130031\n",
      "990 0.8527402965184074 0.7831978031892748 0.8018575851393189\n",
      "0.8568111777444611 0.7848221189195806\n"
     ]
    }
   ],
   "source": [
    "test_label = test_label.to(device)\n",
    "loss_weight = torch.tensor([1.0, 1.904]).to(device)\n",
    "model = MCDHGN(\n",
    "    num_meta_paths=len(gs1),\n",
    "    in_size=features.shape[1],\n",
    "    hidden_size=256,\n",
    "    out_size=2,\n",
    "    num_heads=[4],\n",
    "    dropout=0.4,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "train_mask,val_mask,train_label,val_label = train_batch[0]\n",
    "best_auc = 0\n",
    "best_acc =0\n",
    "best_aupr = 0\n",
    "x_train = torch.cat((train_mask,val_mask),dim=0)\n",
    "y_train = torch.cat((train_label,val_label),dim =0)\n",
    "y_train = y_train.to(device)\n",
    "for epoch in range(1000):\n",
    "    model.train()\n",
    "    _,pred,_,_ = model(gs1,features)\n",
    "    output = pred[x_train]\n",
    "    output = F.log_softmax(output,dim=1)\n",
    "    loss = F.nll_loss(output, y_train,weight=loss_weight)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    output = output.cpu().detach().numpy()\n",
    "    if epoch%10==0:\n",
    "        test_loss,test_acc,test_auc,test_aupr,output,alpha,beta= evaluate(gs1,model,test_mask,test_label)\n",
    "        if test_aupr>best_aupr:\n",
    "            best_auc = test_auc\n",
    "            best_aupr = test_aupr\n",
    "            best_acc=test_acc\n",
    "            fpred = output\n",
    "            fbeta = beta\n",
    "        print(epoch,test_auc,test_aupr,test_acc)\n",
    "myauc,myaupr,fpr,tpr,p,r = generate_curve(fpred,test_mask,test_label)\n",
    "print(myauc,myaupr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
