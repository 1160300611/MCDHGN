{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import dgl\n",
    "import ast\n",
    "from model import MCDHGN\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn.pytorch import GATConv\n",
    "import logging \n",
    "import datetime\n",
    "from model import MySampler\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import EarlyStopping, setup_seed, generate_traning_batch,HeteroDotProductPredictor\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "dgl.random.seed(42)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_paths = [['gg'],['ga','ag'],['gb','bg'],['gc','cg'],['gd','dg'],['ge','eg'],['gf','fg'],['gh','hg'],['gi','ig']]\n",
    "graph_path = './data/network/hetero/new_9nodes_graph.bin'\n",
    "graphs,_ = dgl.data.utils.load_graphs(graph_path)\n",
    "g = graphs[0]\n",
    "features = g.ndata['feature']['Gene']\n",
    "#In order to ensure reproducible operation, in addition to fixing the random number seed, I also fixed the intermediate random walk sampling results and the division results of the training set, verification set, and test set.\n",
    "# you also can get the random sampler result by the codes below.\n",
    "# gene_ids = g.nodes('Gene')\n",
    "# my_sampler = MySampler(g, meta_paths, 128)\n",
    "# _,gs = my_sampler.sample_blocks(gene_ids)\n",
    "# print(gs)\n",
    "gs = []\n",
    "for i in range(9):\n",
    "    loadpath = './Intermediate/blocks/{}gs.bin'.format(i)\n",
    "    with open(loadpath,'rb')as f:\n",
    "        tg = pickle.load(f)\n",
    "        gs.append(tg)\n",
    "# you can generate another train \\ val \\ test \\ set by the codes below\n",
    "# postivefile = './data/label/now_pos427.pkl'\n",
    "# negtivefile = './data/label/now_neg427.pkl'\n",
    "# train_batch,test_mask,test_label = generate_traning_batch(postivefile,negtivefile)\n",
    "test_mask = torch.load('./Intermediate/label_set/mydatatest_mask.pt')\n",
    "test_label = torch.load('./Intermediate/label_set/mydatatest_label.pt')\n",
    "with open('./Intermediate/label_set/train_batch.pkl','rb')as f:\n",
    "    train_batch = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n",
      "Device name: Tesla V100-SXM2-32GB\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(device))\n",
    "else:\n",
    "    print('No GPU available, using CPU instead.')\n",
    "gs1 = [block.to(device) for block in gs]\n",
    "features = features.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(g,model, mask,label):\n",
    "    model.eval() \n",
    "    with torch.no_grad():\n",
    "        _,pred,alpha,beta = model(g,features)\n",
    "        output = pred[mask]\n",
    "        output = F.log_softmax(output,dim=1)\n",
    "        val_loss = F.nll_loss(output, label)\n",
    "        output = output.cpu().numpy()\n",
    "        label = label.cpu().numpy()\n",
    "        predicted_labels = np.argmax(output, axis=1)\n",
    "        acc = accuracy_score(label, predicted_labels)\n",
    "        micro_f1 = f1_score(label, predicted_labels, average='micro')\n",
    "        macro_f1 = f1_score(label, predicted_labels, average='macro')\n",
    "        output = torch.sigmoid(pred[mask]).cpu().detach().numpy()\n",
    "        auc = roc_auc_score(label, output[:, 1])\n",
    "        aupr = average_precision_score(label, output[:, 1])   \n",
    "    return val_loss,acc,auc,aupr,pred.cpu(),alpha,beta,micro_f1,macro_f1\n",
    "def train(g,model,train_mask,label,epochs):\n",
    "    model.train()\n",
    "    link_pred = HeteroDotProductPredictor()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "    for epoch in range(epochs+1):\n",
    "        embeddings,pred,_,_ = model(g,features)\n",
    "        optimizer.zero_grad()\n",
    "        output = pred[train_mask]\n",
    "        output = F.log_softmax(output,dim=1)\n",
    "        loss = F.nll_loss(output, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_weight = torch.tensor([1.0, 1.904]).to(device)\n",
    "logging.basicConfig(filename='train_and_val2024.log', level=logging.INFO)\n",
    "test_label = test_label.to(device)\n",
    "start_time = datetime.datetime.now()\n",
    "logging.info(f'Current datetime: {start_time}')\n",
    "auc = [];aupr = [];acc = [];micro = [];macro = []\n",
    "for i,batch in enumerate(train_batch):\n",
    "    cnt = 0\n",
    "    best_val_auc = 0\n",
    "    best_val_aupr = 0\n",
    "    best_val_acc = 0\n",
    "    best_val_micro_f1 = 0\n",
    "    best_val_macro_f1 = 0\n",
    "    model = MCDHGN(\n",
    "        num_meta_paths=len(gs1),\n",
    "        in_size=features.shape[1],\n",
    "        hidden_size=256,\n",
    "        out_size=2,\n",
    "        num_heads=[4],\n",
    "        dropout=0.4,\n",
    "    ).to(device)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "    for epoch in range(1000):\n",
    "        model.train()\n",
    "        train_mask,val_mask,train_label,val_label = batch\n",
    "        train_mask = train_mask.to(device)\n",
    "        val_mask = val_mask.to(device)\n",
    "        train_label = train_label.to(device)\n",
    "        val_label = val_label.to(device)\n",
    "        l1_regularization = 0\n",
    "        _,pred,_,_ = model(gs1,features)\n",
    "        output = pred[train_mask]\n",
    "        output = F.log_softmax(output,dim=1)\n",
    "        loss = F.nll_loss(output, train_label,weight = loss_weight)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        output = output.cpu().detach().numpy()\n",
    "        train_label = train_label.cpu().numpy()\n",
    "        if(epoch%10 == 0):\n",
    "            cnt=cnt+1\n",
    "            val_loss,val_acc,val_auc,val_aupr,output,_,beta,val_micro,val_macro= evaluate(gs1,model,val_mask,val_label)\n",
    "            if val_auc>best_val_auc:\n",
    "                best_val_auc = val_auc\n",
    "                cnt = 0\n",
    "            if val_aupr >best_val_aupr:\n",
    "                best_val_aupr = val_aupr\n",
    "                cnt = 0\n",
    "            if val_acc>best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "            if val_micro > best_val_micro_f1:\n",
    "                best_val_micro_f1 = val_micro\n",
    "            if val_macro > best_val_macro_f1:\n",
    "                best_val_macro_f1 = val_macro\n",
    "    end_time = datetime.datetime.now()\n",
    "    elapsed_time = end_time - start_time\n",
    "    hours, remainder = divmod(elapsed_time.total_seconds(), 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    logging.info(f\"{i}fold程序运行时间：{int(hours)}小时{int(minutes)}分钟{int(seconds)}秒\")\n",
    "    logging.info(f\"auc{best_val_auc},aupr{best_val_aupr}\")\n",
    "    auc.append(best_val_auc)\n",
    "    aupr.append(best_val_aupr)\n",
    "    acc.append(best_val_acc)\n",
    "    macro.append(best_val_macro_f1)\n",
    "    micro.append(best_val_micro_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f\"mean: auc{np.mean(auc)},+- {np.std(auc)}\")\n",
    "logging.info(f\"mean: aupr{np.mean(aupr)},+- {np.std(aupr)}\")\n",
    "logging.info(f\"mean: acc{np.mean(acc)},+- {np.std(acc)}\")\n",
    "logging.info(f\"mean: micro_f1{np.mean(micro)},+- {np.std(micro)}\")\n",
    "logging.info(f\"mean: macro_f1{np.mean(macro)},+- {np.std(macro)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
